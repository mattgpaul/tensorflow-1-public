{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W4/ungraded_labs/C3_W4_Lab_2_irish_lyrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqIxQYm8h06Z"
   },
   "source": [
    "# Ungraded Lab: Generating Text from Irish Lyrics\n",
    "\n",
    "In the previous lab, you trained a model on just a single song. You might have found that the output text can quickly become gibberish or repetitive. Even if you tweak the hyperparameters, the model will still be limited by its vocabulary of only 263 words. The model will be more flexible if you train it on a much larger corpus and that's what you'll be doing in this lab. You will use lyrics from more Irish songs then see how the generated text looks like. You will also see how this impacts the process from data preparation to model training. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb1mfOvch4Sv"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmBFI788pOXx"
   },
   "source": [
    "## Building the Word Vocabulary\n",
    "\n",
    "You will first download the lyrics dataset. These will be from a compilation of traditional Irish songs and you can see them [here](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W4/misc/Laurences_generated_poetry.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pylt5qZYsWPh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-31 20:09:35--  https://storage.googleapis.com/tensorflow-1-public/course3/irish-lyrics-eof.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.72.80, 142.250.72.48, 142.250.69.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.72.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68970 (67K) [text/plain]\n",
      "Saving to: ‘irish-lyrics-eof.txt’\n",
      "\n",
      "irish-lyrics-eof.tx 100%[===================>]  67.35K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2022-08-31 20:09:36 (4.48 MB/s) - ‘irish-lyrics-eof.txt’ saved [68970/68970]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course3/irish-lyrics-eof.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v6JYQGNPXCW"
   },
   "source": [
    "Next, you will lowercase and split the plain text into a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKOO7DFCPX3L"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = open('./irish-lyrics-eof.txt').read()\n",
    "\n",
    "# Lowercase and split the text\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "# Preview the result\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkP2CP0qP8RD"
   },
   "source": [
    "From here, you can initialize the `Tokenizer` class and generate the word index dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRnDnCW-Z7qv"
   },
   "outputs": [],
   "source": [
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Define the total words. You add 1 for the index `0` which is just the padding token.\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f'word index dictionary: {tokenizer.word_index}')\n",
    "print(f'total words: {total_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK29FzZ7QW-4"
   },
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Next, you will generate the inputs and labels for your model. The process will be identical to the previous lab. The `xs` or inputs to the model will be padded sequences, while the `ys` or labels are one-hot encoded arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "soPGVheskaQP"
   },
   "outputs": [],
   "source": [
    "# Initialize the sequences list\n",
    "input_sequences = []\n",
    "\n",
    "# Loop over every line\n",
    "for line in corpus:\n",
    "\n",
    "\t# Tokenize the current line\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "\t# Loop over the line several times to generate the subphrases\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\t\n",
    "\t\t# Generate the subphrase\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\n",
    "\t\t# Append the subphrase to the sequences list\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Get the length of the longest line\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create inputs and label by splitting the last token in the subphrases\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "# Convert the label into one-hot arrays\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmWHCO0dQGlZ"
   },
   "source": [
    "You can then print some of the examples as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pJtwVB2NbOAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence: ['come', 'all', 'ye', 'maidens', 'young', 'and', 'fair']\n",
      "[51, 12, 96, 1217, 48, 2, 69]\n"
     ]
    }
   ],
   "source": [
    "# Get sample sentence\n",
    "sentence = corpus[0].split()\n",
    "print(f'sample sentence: {sentence}')\n",
    "\n",
    "# Initialize token list\n",
    "token_list = []\n",
    "\n",
    "# Look up the indices of each word and append to the list\n",
    "for word in sentence: \n",
    "  token_list.append(tokenizer.word_index[word])\n",
    "\n",
    "# Print the token list\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lMr6kKfzROlW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list: [   0    0    0    0    0    0    0    0    0   51   12   96 1217   48\n",
      "    2]\n",
      "decoded to text: ['come all ye maidens young and']\n",
      "one-hot label: [0. 0. 0. ... 0. 0. 0.]\n",
      "index of label: 69\n"
     ]
    }
   ],
   "source": [
    "# Pick element\n",
    "elem_number = 5\n",
    "\n",
    "# Print token list and phrase\n",
    "print(f'token list: {xs[elem_number]}')\n",
    "print(f'decoded to text: {tokenizer.sequences_to_texts([xs[elem_number]])}')\n",
    "\n",
    "# Print label\n",
    "print(f'one-hot label: {ys[elem_number]}')\n",
    "print(f'index of label: {np.argmax(ys[elem_number])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "49Cv68JOakwv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list: [   0    0    0    0    0    0    0    0    0    0   51   12   96 1217\n",
      "   48]\n",
      "decoded to text: ['come all ye maidens young']\n",
      "one-hot label: [0. 0. 1. ... 0. 0. 0.]\n",
      "index of label: 2\n"
     ]
    }
   ],
   "source": [
    "# Pick element\n",
    "elem_number = 4\n",
    "\n",
    "# Print token list and phrase\n",
    "print(f'token list: {xs[elem_number]}')\n",
    "print(f'decoded to text: {tokenizer.sequences_to_texts([xs[elem_number]])}')\n",
    "\n",
    "# Print label\n",
    "print(f'one-hot label: {ys[elem_number]}')\n",
    "print(f'index of label: {np.argmax(ys[elem_number])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKWWUZm5VPG9"
   },
   "source": [
    "## Build and compile the Model\n",
    "\n",
    "Next, you will build and compile the model. We placed some of the hyperparameters at the top of the code cell so you can easily tweak it later if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w9vH8Y59ajYL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 20:11:31.612525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.639206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.639371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.640239: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-31 20:11:31.640878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.641007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.641124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.950086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.950248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.950412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 20:11:31.950520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6759 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:2b:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 15, 100)           269000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 300)              301200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2690)              809690    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,379,890\n",
      "Trainable params: 1,379,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "lstm_units = 150\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "          Embedding(total_words, embedding_dim, input_length=max_sequence_len-1),\n",
    "          Bidirectional(LSTM(lstm_units)),\n",
    "          Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "# Use categorical crossentropy because this is a multi-class problem\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpI0d9cfR43c"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "From the model summary above, you'll notice that the number of trainable params is much larger than the one in the previous lab. Consequently, that usually means a slower training time. It will take roughly 7 seconds per epoch with the GPU enabled in Colab and you'll reach around 76% accuracy after 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Nc4zC7C4jJpN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 11/377 [..............................] - ETA: 1s - loss: 7.3348 - accuracy: 0.0284       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 20:11:47.414690: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377/377 [==============================] - 4s 5ms/step - loss: 6.6340 - accuracy: 0.0743\n",
      "Epoch 2/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 5.7679 - accuracy: 0.1147\n",
      "Epoch 3/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 4.9025 - accuracy: 0.1663\n",
      "Epoch 4/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 3.9352 - accuracy: 0.2385\n",
      "Epoch 5/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 3.0927 - accuracy: 0.3444\n",
      "Epoch 6/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 2.4268 - accuracy: 0.4523\n",
      "Epoch 7/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.8872 - accuracy: 0.5626\n",
      "Epoch 8/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.5362 - accuracy: 0.6341\n",
      "Epoch 9/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.2993 - accuracy: 0.6842\n",
      "Epoch 10/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.1671 - accuracy: 0.7170\n",
      "Epoch 11/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0809 - accuracy: 0.7377\n",
      "Epoch 12/100\n",
      "377/377 [==============================] - 2s 6ms/step - loss: 1.0266 - accuracy: 0.7504\n",
      "Epoch 13/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0998 - accuracy: 0.7232\n",
      "Epoch 14/100\n",
      "377/377 [==============================] - 2s 6ms/step - loss: 1.1998 - accuracy: 0.6958\n",
      "Epoch 15/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.1827 - accuracy: 0.6970\n",
      "Epoch 16/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0684 - accuracy: 0.7264\n",
      "Epoch 17/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9981 - accuracy: 0.7466\n",
      "Epoch 18/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9185 - accuracy: 0.7656\n",
      "Epoch 19/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8747 - accuracy: 0.7775\n",
      "Epoch 20/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8966 - accuracy: 0.7716\n",
      "Epoch 21/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9745 - accuracy: 0.7470\n",
      "Epoch 22/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0655 - accuracy: 0.7255\n",
      "Epoch 23/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0449 - accuracy: 0.7321\n",
      "Epoch 24/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9879 - accuracy: 0.7417\n",
      "Epoch 25/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9552 - accuracy: 0.7512\n",
      "Epoch 26/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8915 - accuracy: 0.7671\n",
      "Epoch 27/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8764 - accuracy: 0.7733\n",
      "Epoch 28/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8858 - accuracy: 0.7682\n",
      "Epoch 29/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9781 - accuracy: 0.7415\n",
      "Epoch 30/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0235 - accuracy: 0.7323\n",
      "Epoch 31/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9609 - accuracy: 0.7474\n",
      "Epoch 32/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9316 - accuracy: 0.7544\n",
      "Epoch 33/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8388 - accuracy: 0.7822\n",
      "Epoch 34/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8087 - accuracy: 0.7883\n",
      "Epoch 35/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8522 - accuracy: 0.7711\n",
      "Epoch 36/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9066 - accuracy: 0.7615\n",
      "Epoch 37/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9389 - accuracy: 0.7509\n",
      "Epoch 38/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9741 - accuracy: 0.7440\n",
      "Epoch 39/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0905 - accuracy: 0.7235\n",
      "Epoch 40/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.1126 - accuracy: 0.7131\n",
      "Epoch 41/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9903 - accuracy: 0.7368\n",
      "Epoch 42/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8886 - accuracy: 0.7682\n",
      "Epoch 43/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8775 - accuracy: 0.7682\n",
      "Epoch 44/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8211 - accuracy: 0.7839\n",
      "Epoch 45/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.7969 - accuracy: 0.7895\n",
      "Epoch 46/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8386 - accuracy: 0.7794\n",
      "Epoch 47/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8896 - accuracy: 0.7606\n",
      "Epoch 48/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9440 - accuracy: 0.7500\n",
      "Epoch 49/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9556 - accuracy: 0.7450\n",
      "Epoch 50/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9684 - accuracy: 0.7440\n",
      "Epoch 51/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9249 - accuracy: 0.7554\n",
      "Epoch 52/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8630 - accuracy: 0.7719\n",
      "Epoch 53/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8275 - accuracy: 0.7775\n",
      "Epoch 54/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8130 - accuracy: 0.7829\n",
      "Epoch 55/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8174 - accuracy: 0.7804\n",
      "Epoch 56/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8635 - accuracy: 0.7712\n",
      "Epoch 57/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9167 - accuracy: 0.7545\n",
      "Epoch 58/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9433 - accuracy: 0.7473\n",
      "Epoch 59/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9355 - accuracy: 0.7497\n",
      "Epoch 60/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9073 - accuracy: 0.7572\n",
      "Epoch 61/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8545 - accuracy: 0.7691\n",
      "Epoch 62/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8403 - accuracy: 0.7769\n",
      "Epoch 63/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8421 - accuracy: 0.7753\n",
      "Epoch 64/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8218 - accuracy: 0.7814\n",
      "Epoch 65/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8061 - accuracy: 0.7836\n",
      "Epoch 66/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8519 - accuracy: 0.7731\n",
      "Epoch 67/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8502 - accuracy: 0.7736\n",
      "Epoch 68/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8515 - accuracy: 0.7745\n",
      "Epoch 69/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8951 - accuracy: 0.7657\n",
      "Epoch 70/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9422 - accuracy: 0.7480\n",
      "Epoch 71/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9192 - accuracy: 0.7535\n",
      "Epoch 72/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8726 - accuracy: 0.7673\n",
      "Epoch 73/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8454 - accuracy: 0.7725\n",
      "Epoch 74/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.7827 - accuracy: 0.7889\n",
      "Epoch 75/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.7952 - accuracy: 0.7876\n",
      "Epoch 76/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8125 - accuracy: 0.7842\n",
      "Epoch 77/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8173 - accuracy: 0.7799\n",
      "Epoch 78/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8089 - accuracy: 0.7779\n",
      "Epoch 79/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8270 - accuracy: 0.7759\n",
      "Epoch 80/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8894 - accuracy: 0.7632\n",
      "Epoch 81/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9938 - accuracy: 0.7419\n",
      "Epoch 82/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0316 - accuracy: 0.7348\n",
      "Epoch 83/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9763 - accuracy: 0.7396\n",
      "Epoch 84/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8765 - accuracy: 0.7634\n",
      "Epoch 85/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8114 - accuracy: 0.7841\n",
      "Epoch 86/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.7462 - accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.7126 - accuracy: 0.8078\n",
      "Epoch 88/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.7748 - accuracy: 0.7928\n",
      "Epoch 89/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8099 - accuracy: 0.7868\n",
      "Epoch 90/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9559 - accuracy: 0.7509\n",
      "Epoch 91/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0005 - accuracy: 0.7370\n",
      "Epoch 92/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9274 - accuracy: 0.7515\n",
      "Epoch 93/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 1.0853 - accuracy: 0.7255\n",
      "Epoch 94/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9388 - accuracy: 0.7559\n",
      "Epoch 95/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9449 - accuracy: 0.7604\n",
      "Epoch 96/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.9216 - accuracy: 0.7622\n",
      "Epoch 97/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8512 - accuracy: 0.7770\n",
      "Epoch 98/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8181 - accuracy: 0.7834\n",
      "Epoch 99/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.8142 - accuracy: 0.7848\n",
      "Epoch 100/100\n",
      "377/377 [==============================] - 2s 5ms/step - loss: 0.7915 - accuracy: 0.7917\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(xs, ys, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgAzLnLATFts"
   },
   "source": [
    "You can visualize the accuracy below to see how it fluctuates as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3YXGelKThoTT"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()\n",
    "\n",
    "# Visualize the accuracy\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gxKIcvGTUnw"
   },
   "source": [
    "## Generating Text\n",
    "\n",
    "Now you can let the model make its own songs or poetry! Because it is trained on a much larger corpus, the results below should contain less repetitions as before. The code below picks the next word based on the highest probability output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help me obi-wan kinobi youre my only hope and green were brave are they your friend gone gone gone gone gone your eyes hush love love i love gone love be eyes and town i town love gone away from town gone love eyes tree my love gone she right till my love love writin love gone gone gone i love eyes i gone away gone white eyes any lily man i could i love love gone among mad at i love gone she right gone gone among i right my keeping get love gone gone then i love best with sinking eyes gone alas gone or left\n"
     ]
    }
   ],
   "source": [
    "# Define seed text\n",
    "seed_text = \"help me obi-wan kinobi youre my only hope\"\n",
    "\n",
    "# Define total words to predict\n",
    "next_words = 100\n",
    "\n",
    "# Loop until desired length is reached\n",
    "for _ in range(next_words):\n",
    "\n",
    "\t# Convert the seed text to a token sequence\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "\t# Pad the sequence\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t\n",
    "\t# Feed to the model and get the probabilities for each index\n",
    "\tprobabilities = model.predict(token_list)\n",
    "\n",
    "\t# Get the index with the highest probability\n",
    "\tpredicted = np.argmax(probabilities, axis=-1)[0]\n",
    "\n",
    "\t# Ignore if index is 0 because that is just the padding.\n",
    "\tif predicted != 0:\n",
    "\t\t\n",
    "\t\t# Look up the word associated with the index. \n",
    "\t\toutput_word = tokenizer.index_word[predicted]\n",
    "\n",
    "\t\t# Combine with the seed text\n",
    "\t\tseed_text += \" \" + output_word\n",
    "\n",
    "# Print the result\t\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHtrtAFAT6tn"
   },
   "source": [
    "Here again is the code that gets the top 3 predictions and picks one at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yJfzKm-8mVKD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help me obi-wan kinobi youre my only hope to ireland faded boy are so strength i would goes at gone i love beggarman thy eyes your eyes gone i love gone love someone i safely gone and year love love grew gone i roved then then eyes bridle town gone forbid went your goes high high i boy in this proud eyes glisten back to play the fine groves love love hands love gone love love gone love til da dying white i now seems eyes dirty took from town and odonnell eyes and bee jollity town tree hat right steal love your unseen clouded ever i sinking\n"
     ]
    }
   ],
   "source": [
    "# Define seed text\n",
    "seed_text = \"help me obi-wan kinobi youre my only hope\"\n",
    "\n",
    "# Define total words to predict\n",
    "next_words = 100\n",
    "\n",
    "# Loop until desired length is reached\n",
    "for _ in range(next_words):\n",
    "\n",
    "\t# Convert the seed text to a token sequence\n",
    "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "\t# Pad the sequence\n",
    "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t\n",
    "\t# Feed to the model and get the probabilities for each index\n",
    "  probabilities = model.predict(token_list)\n",
    "\n",
    "  # Pick a random number from [1,2,3]\n",
    "  choice = np.random.choice([1,2,3])\n",
    "\t\n",
    "  # Sort the probabilities in ascending order \n",
    "  # and get the random choice from the end of the array\n",
    "  predicted = np.argsort(probabilities)[0][-choice]\n",
    "\n",
    "\t# Ignore if index is 0 because that is just the padding.\n",
    "  if predicted != 0:\n",
    "\t\t\n",
    "\t\t# Look up the word associated with the index. \n",
    "\t  output_word = tokenizer.index_word[predicted]\n",
    "\n",
    "\t\t# Combine with the seed text\n",
    "\t  seed_text += \" \" + output_word\n",
    "\n",
    "# Print the result\t\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DP0--sdMUJ_k"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "This lab shows the effect of having a larger dataset to train your text generation model. As expected, this will take a longer time to prepare and train but the output will less likely become repetitive or gibberish. Try to tweak the hyperparameters and see if you get better results. You can also find some other text datasets and use it to train the model here.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W4_Lab_2_irish_lyrics.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tensorflow-1-public': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "48fec8ba1c20c009a355e2e354d6bac7b3b87e7ce91292f937ebe5a547831829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
